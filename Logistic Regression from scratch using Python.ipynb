{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ce709c",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Logisitic Regression}}$<br>\n",
    "***\n",
    "\n",
    "Logisitic Regression is the supervised statistical technique to find the probability of dependent variable. Logistic Regression helps to find relationship between the dependent variable and independent variables by predicting the probabilities or chances of occurrence. Sigmoid functions convert the probabilities into binary values which could be further used for predictions.\n",
    "\n",
    "From linear regression, we know that the equation for linear function is given as:\n",
    "\\begin{align}\n",
    "f(w,b) = wx + b\n",
    "\\end{align}\n",
    "\n",
    "However in logistic regression, we do not want continuous values, we want probabilities. To get the probabilities, we apply sigmoid function to the linear model.\n",
    "\n",
    "\\begin{align}\n",
    "s(x) = \\frac{1}{1 + e^{-wx+b}}\n",
    "\\end{align}\n",
    "\n",
    "##### Cost Function\n",
    "\n",
    "To get the optimal values of weights(w) and bias(b), we use gradient descent. To get to the gradient descent, let us first calculate the cost function. In logistic regression, cost function is the cross entropy function and is given as:\n",
    "\n",
    "\\begin{align}\n",
    "J(w,b) = J(θ) = \\frac{1}{N} \\sum_{i=1}^{n} [y_{i} log(h_{θ}x_{i}) + (1 - y^{i})log(1 - h_{θ}x_{i})]\n",
    "\\end{align}\n",
    "\n",
    "The update rules for w and b is given as:\n",
    "\\begin{align}\n",
    "w = w - α dw\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "b = b - α db\n",
    "\\end{align}\n",
    "\n",
    "Formula for derivatives can be given as:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dJ}{dw} = \\frac{1}{N} \\sum_{}^{} 2 x_{i}(ŷ - y_{i})\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dJ}{db} = \\frac{1}{N} \\sum_{}^{} 2 (ŷ - y_{i})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8d830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b73535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressions():\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, n_iters = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            # implementing the linear function equation\n",
    "            linear_function = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # calculating y predicted by applying the sigmoid function\n",
    "            y_predicted = self._sigmoid(linear_function)\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1/n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        linear_function = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_function)\n",
    "        y_predicted_prob = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_prob)\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c03960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "    \n",
    "    bc = load_breast_cancer()\n",
    "    \n",
    "    X = bc.data\n",
    "    y = bc.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "    \n",
    "    log_reg = LogisticRegressions(learning_rate = 0.0001, n_iters = 1000)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = log_reg.predict(X_test)\n",
    "    \n",
    "    print(\"The accuracy score is: \", accuracy(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
